{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "S&P 500 Analytics - data loading.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPzc9cI55HD4j0530eCAA9Q",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/maggawron/S_P500/blob/main/S%26P_500_Analytics_data_loading.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k0HubBwKKjQj"
      },
      "source": [
        "#**S&P 500 Analytics - data loading** *by Magdalena Gawron*\n",
        "Welcome to the first part of my S&P 500 analytics project. This part will focus on fetching stock tickers and names, historical price data and analyst recommendations via webscraping and Yahoo APIs \n",
        "\n",
        "All the data structures generated in this notebook have been saved on my github repository in the [Data](https://github.com/maggawron/S_P500/tree/main/Data) folder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xjzVeqS1KZ7v"
      },
      "source": [
        "#Imports \n",
        "Import all the key packages needed to fetch the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UcdAM6aGKLNy",
        "outputId": "584b4a48-924a-4247-bc5a-a1b43c009c47"
      },
      "source": [
        "#@title Imports\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import sys\n",
        "from datetime import date\n",
        "from copy import deepcopy\n",
        "\n",
        "!pip install yfinance\n",
        "!pip install yahoofinancials\n",
        "!pip install Yahoo-ticker-downloader\n",
        "\n",
        "import bs4 as bs\n",
        "import pickle\n",
        "import requests\n",
        "import yfinance as yf\n",
        "from yahoofinancials import YahooFinancials\n",
        "\n",
        "import pickle\n",
        "\n",
        "from datetime import timedelta\n",
        "\n",
        "from google.colab import output\n",
        "output.clear()\n",
        "\n",
        "print(\"done\")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pKmU0FAVMqiQ"
      },
      "source": [
        "#Get all basic data about S&P 500 stocks\n",
        "Scrape stocks' tickers and stocks full names from Wikipedia page.\n",
        "This process can be skipped via using the next Colab file with analytics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZnLLOSaYM9DD"
      },
      "source": [
        "#@title Scrape all SP500 tickers from Wikipedia page\n",
        "def sp500_tickers():\n",
        "  \"\"\"scrape all S&P 500 tickers from Wikipedia and return them as a list\"\"\"\n",
        "  resp = requests.get('http://en.wikipedia.org/wiki/List_of_S%26P_500_companies')\n",
        "  soup = bs.BeautifulSoup(resp.text, 'lxml')\n",
        "  table = soup.find('table', {'class': 'wikitable sortable'})\n",
        "  tickers = []\n",
        "  all_names = []\n",
        "  for row in table.findAll('tr')[1:]:\n",
        "    ticker = row.findAll('td')[0].text\n",
        "    name = row.findAll('td')[1].text\n",
        "    all_names.append(name.strip())\n",
        "    tickers.append(ticker.strip())\n",
        "        \n",
        "  with open(\"sp500tickers.pickle\",\"wb\") as f:\n",
        "    pickle.dump(tickers,f)     \n",
        "  return tickers, all_names\n",
        "\n",
        "assets, stock_names = sp500_tickers()\n",
        "\n",
        "#Remove couple of tickers triggering YFinance API errors   \n",
        "to_pop = [\"BRK.B\",\"BF.B\", \"CI\", \"STE\", \"ZBH\"]\n",
        "for i in to_pop:\n",
        "  ind = assets.index(i)\n",
        "  assets.pop(ind)\n",
        "  stock_names.pop(ind)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tPMxPUZuPpLT"
      },
      "source": [
        "#Save the data to a file\n",
        "with open('assets_tickers.pkl', 'wb') as f:\n",
        "  pickle.dump(assets, f)\n",
        "with open('assets_names.pkl', 'wb') as f:\n",
        "  pickle.dump(stock_names, f)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F6mrCGP6NJYw"
      },
      "source": [
        "#Download the US stock prices data\n",
        "This process takes quite some time to complete. It can be skipped via using the next Colab file with analytics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kmb9zgQ8NKKW"
      },
      "source": [
        "#@title Download stock price data for S&P 500 until today\n",
        "date_start = '2013-01-01' #@param \n",
        "date_end = str(date.today() - timedelta(days=1))\n",
        "interval = 'daily' #@param \n",
        "\n",
        "#Generate a dataframe with all stock prices \n",
        "all_data = pd.DataFrame({\"date\":[str(date.today())]})\n",
        "\n",
        "for ticker in assets:\n",
        "  yahoo_financials = YahooFinancials(ticker)\n",
        "  data = yahoo_financials.get_historical_price_data(start_date=date_start, \n",
        "                                                    end_date=date_end, \n",
        "                                                    time_interval=interval)\n",
        "  \n",
        "  df = pd.DataFrame(data[ticker]['prices'])\n",
        "  prices_df = pd.DataFrame({\"date\": df[\"formatted_date\"], ticker: df['adjclose']}) \n",
        "  all_data = pd.merge(all_data, prices_df, how=\"outer\", on=\"date\")\n",
        "  \n",
        "#All_data is the dataframe containing all S&P 500 stock prices available on Yahoo platform"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NTvnaghLNMuQ",
        "outputId": "9be82706-f90c-47b0-8e5b-7fa8d16fdf40"
      },
      "source": [
        "#@title Pick 80% of the stocks with the longest presence on the stock market\n",
        "#Exclude all stocks with short track record\n",
        "mean_no_records = all_data.nunique().quantile(0.2)\n",
        "\n",
        "for ticker in assets:\n",
        "  if ticker in all_data:\n",
        "    if all_data[ticker].nunique() < mean_no_records:\n",
        "      all_data.drop(columns = ticker, inplace=True)\n",
        "\n",
        "all_data.dropna(inplace=True)\n",
        "all_data.shape"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1963, 401)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Mxy2dciNRC8"
      },
      "source": [
        "#Save the data to a file\n",
        "price_data = all_data.to_csv()\n",
        "with open(\"price_data.csv\", \"w\") as f:\n",
        "  f.write(price_data)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QlRvvvm-O8pQ"
      },
      "source": [
        "#Download all available rating agencies recommendations\n",
        "This process takes quite some time to complete. It can be skipped via using the next Colab file with analytics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IidJ-nORO9oW",
        "outputId": "e7777927-b71b-40e8-d177-ae76db661cca"
      },
      "source": [
        "#@title Analyst rating dataframe\n",
        "\n",
        "#Recomendations will be fetched only for \n",
        "#80% of stocks with longest market presence\n",
        "recom_data = pd.DataFrame()\n",
        "assets = all_data.columns.values\n",
        "assets = np.delete(assets,0)\n",
        "\n",
        "def get_recom_data(recom_data, assets, start, end):\n",
        "  \"\"\"yfinance server limits output length to around 100 queries\"\"\"\n",
        "  \"\"\"so output has to be fetched in batches\"\"\"\n",
        "  for index, ticker in enumerate(assets):\n",
        "    if index >= start and index < end:\n",
        "      yf_symbol = yf.Ticker(ticker)\n",
        "      df_rec = yf_symbol.recommendations\n",
        "      if df_rec is not None: \n",
        "        df_rec['Ticker'] = ticker\n",
        "        df_rec = df_rec[['Ticker', \"Action\", \"Firm\",\n",
        "                         \"To Grade\", \"From Grade\"]]\n",
        "        recom_data = pd.concat([recom_data, df_rec])\n",
        "  return recom_data\n",
        "\n",
        "#Data is fetched in batches of 100 items\n",
        "for i in range(5):\n",
        "  recom_data = get_recom_data(recom_data, assets, i*100, (i+1)*100)\n",
        "  print(f'{(i+1)*100} tickers fetched')"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100 tickers fetched\n",
            "200 tickers fetched\n",
            "300 tickers fetched\n",
            "400 tickers fetched\n",
            "500 tickers fetched\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LMLHkd1iPA7I"
      },
      "source": [
        "#Save the data to the file\n",
        "recom_csv = recom_data.to_csv()\n",
        "with open(\"recom_csv.csv\", \"w\") as f:\n",
        "  f.write(recom_csv)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lBiCtuAQPFdD"
      },
      "source": [
        "#Download all the stocks' number of shares outstanding \n",
        "This process takes quite some time to complete. It can be skipped via using the next Colab file with analytics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GTdb2zi1PKts",
        "outputId": "fa116fb1-f3a5-47e9-91fc-9afe05fd6f22"
      },
      "source": [
        "shares_now = {}\n",
        "\n",
        "def fetch_shares(assets, shares_now):\n",
        "  for ticker in assets:\n",
        "    ticker_data = yf.Ticker(ticker)\n",
        "    s_out = ticker_data.info\n",
        "    if \"sharesOutstanding\" in s_out:\n",
        "      shares_now[ticker] = s_out[\"sharesOutstanding\"]\n",
        "  return shares_now\n",
        "\n",
        "for i in range(5):\n",
        "  shares_now = fetch_shares(assets[i*100:(i+1)*100], shares_now)\n",
        "  print(f\"{(i+1)*100} tickers done\")\n",
        "\n",
        "print(shares_now)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100 tickers done\n",
            "200 tickers done\n",
            "300 tickers done\n",
            "400 tickers done\n",
            "500 tickers done\n",
            "{'MMM': 576822016, 'ABT': 1772359936, 'ABBV': 1764829952, 'ABMD': 45189900, 'ACN': 635000000, 'ATVI': 772857024, 'ADBE': 479719008, 'AMD': 1202710016, 'AAP': 67854496, 'AES': 665131008, 'AFL': 702444992, 'A': 308310016, 'APD': 220894000, 'AKAM': 162704992, 'ALK': 123664000, 'ALB': 106457000, 'ARE': 134944000, 'ALXN': 218844992, 'ALGN': 78850400, 'ALLE': 92039000, 'LNT': 249644000, 'ALL': 304068000, 'GOOGL': 300644000, 'GOOG': 329867008, 'MO': 1858419968, 'AMZN': 500889984, 'AMCR': 1568480000, 'AEE': 247207008, 'AAL': 566489984, 'AEP': 496390016, 'AXP': 805201984, 'AIG': 861526016, 'AMT': 444212992, 'AWK': 181272000, 'AMP': 117978000, 'ABC': 204142000, 'AME': 230064992, 'AMGN': 582169024, 'APH': 299155008, 'ADI': 369560000, 'ANSS': 85884096}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hji_GFVEPLos"
      },
      "source": [
        "#Save the data to a file\n",
        "with open('shares_outstanding.pkl', 'wb') as f:\n",
        "  pickle.dump(shares_now, f)"
      ],
      "execution_count": 17,
      "outputs": []
    }
  ]
}